---
title: "Bayespmtools Package Tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{bayespmtools_tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(bayespmtools)
```

## Introduction

Current sample size calculations for external validation of risk prediction models require researchers to have fixed values of assumed model performance metrics alongside target precision levels, which makes it difficult to represent a complete picture.

Using a Bayesian framework enables researchers to have more flexibility for sample size rules based on expected precision, assurance probabilities, and VoI.

The BayesPMTools R package incorporates the implementation of our proposed Bayesian approach towards Riley's multi-criteria sample size calculations. Detauls of this approach are provided in <ref to the preprint>.

This tutorial overviews two core functions in this R package: XXX, and YYY. This tutorial assumes the BayesPMTools package is installed on your computer. Please refer to <package home page for details>[ref to package on github]


### The ISARIC Model

As a case study, we present sample size calculations for the ISARIC model, a risk prediction model for deterioration of COVID-19 infection in hospitalized patients [REF]. The ISARIC 4C Model is a risk prediction model for predicting deterioration in patients hospitalized from COVID-19 in UK @gupta2021isaric. London was left out for external validation while the other 8 regions were used in creating the model. 

We assume we plan to validate this study in the London region. The performance of the model in the London region will be a random draw from the distribution of performance observed across the other 8 regions of the UK.



### Computing Evidence and Specifying Targets

The values in the evidence are extracted from meta-analysis of the original report for the ISARIC 4C model, which produced these values based on predictive distribution of the internal-external validation results.

The Bayesian sample size calculation approach requires specifying prior distribution on 1) outcome prevalence, 2) c-statistic, 3) calibration slope, 4) one of calibration intercept, O/E ratio, or mean calibration error. These dsitributions are dervied from a meta-analysis of internal-external validation results from the original study. Details of this meta-analysis are provided elsewhere [REF]. Here, we directly use the predictive distribtutions from this meta-analysis.


```{r set-seed}
set.seed(123)
```

```{r load-evidence}
evidence <- list(prev=list(type="beta", mean=0.427966984132821, sd=0.0295397309129426),
                 cstat=list(type="logitnorm", mean=0.760628336908955, sd=0.00635806041351944),
                 cal_mean=list(type="norm", mean=-0.00934717199436785, sd=0.124517605045825),
                 cal_slp=list(type="norm", mean=0.995017759715243, sd = 0.0237278675967507))
```


Targets are specified based the same as the method proposed in @riley2021minsample, for comparability. The target 95% confidence interval widths are as follows: C-Statistic: 0.10, O/E ratio: 0.22, Calibration Slope: 0.3

```{r targets-samp}
targets_samp <- list(eciw.cstat=0.1,
                eciw.cal_oe=0.22,
                eciw.cal_slp=0.30,
                qciw.cstat=c(0.1, 0.9),
                qciw.cal_oe=c(0.22, 0.9),
                qciw.cal_slp=c(0.30,0.9),
                assurance.nb=0.9)
```

### Sample size calculator (bpm_valsamp())

This is the main function call that computes the sample size requirements based on our evidence and targets. It returns the required sample size for external validation for each parameter requested in the targets.

```{r bpm_valsamp}
res <- bpm_valsamp(evidence=evidence, #Evidence as a list
                   dist_type="logitnorm", #Distribution type for calibrated risks
                   method="sample", #Sample based or tw-level ("2s") method
                   targets=targets_samp, #Targets (as specified above)
                   n_sim=10000, #Number of Monte Carlo simulations
                   threshold=0.2) #Risk threshold for NB VoI calculations
```

```{r print-samp-results}
#Print results
print(res$N)
```

### Computing precision / VoI for a given sample size (bpm_valprec())

This function returns the parameter values based on the given sample sizes and targets. It works as an inverse to the bpm_valsamp function.

For this example, we will be using the outputs from bpm_valsamp to verify that the 95%CI widths and VoI values for sample size components derived from *bpm_valprec* are indeed close to the desired ones. As such, we use the same evidence element defined earlier, and create a new target element target_prec, that contains the same parameters target_samp but defined as T for present eciw targets, or 0.9 for present qciw targets.

The N_prec variable includes the output sample sizes from bpm_valsamp.

```{r}
N_prec = c(351, 430, 1064, 399, 522, 1181, 306)

targets_prec = list(eciw.cstat = T, eciw.cal_oe = T, eciw.cal_slp = T, qciw.cstat = 0.9, qciw.cal_oe = 0.9, qciw.cal_slp = 0.9, assurance.nb=T)

prec <- bpm_valprec(
  N = N_prec, #Sample sizes
  evidence = evidence, #Evidence as a list
  dist_type="logitnorm", #Distribution type for calibrated risks
  method="sample", #Sample based or tw-level ("2s") method
  targets = targets_prec, #Targets specified above
  n_sim = 1000, #Number of Monte Carlo simulations
  threshold=0.2) #Risk threshold for NB VoI calculations)

```

```{r}
#print results
print(prec$eciw)

print(prec$qciw)

print(prec$assurance)
```

### References